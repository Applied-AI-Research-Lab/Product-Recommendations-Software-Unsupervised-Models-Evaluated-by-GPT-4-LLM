{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "k-means *clustering*"
      ],
      "metadata": {
        "id": "JFUg9tPeHOoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import silhouette_score\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive to access and save files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define global variables for TF-IDF transformers\n",
        "title_tfidf = None\n",
        "category_tfidf = None\n",
        "\n",
        "# Transform input data using pre-trained TF-IDF transformers\n",
        "def transform_input(title, category, title_tfidf, category_tfidf):\n",
        "    title_transformed = title_tfidf.transform([title])\n",
        "    category_transformed = category_tfidf.transform([category])\n",
        "    return hstack([title_transformed, category_transformed])\n",
        "\n",
        "# Train the KMeans clustering model\n",
        "def train_model(train_data, val_data):\n",
        "    global title_tfidf, category_tfidf  # Declare as global variables\n",
        "\n",
        "    # Combine training and validation datasets for training\n",
        "    combined_data = pd.concat([train_data, val_data], axis=0, ignore_index=True)\n",
        "\n",
        "    # Split your combined data into features (X) and labels (y)\n",
        "    X_combined = combined_data[['title', 'categoryName']]\n",
        "\n",
        "    # Feature extraction using TF-IDF for title and categoryName\n",
        "    title_tfidf = TfidfVectorizer().fit(X_combined['title'])\n",
        "    category_tfidf = TfidfVectorizer().fit(X_combined['categoryName'])\n",
        "\n",
        "    # Combine features into a single matrix for training\n",
        "    title_transformed = title_tfidf.transform(X_combined['title'])\n",
        "    category_transformed = category_tfidf.transform(X_combined['categoryName'])\n",
        "    X_combined_transformed = hstack([title_transformed, category_transformed])\n",
        "\n",
        "    # Choose the number of clusters\n",
        "    num_clusters = 10\n",
        "    model = make_pipeline(KMeans(n_clusters=num_clusters))\n",
        "\n",
        "    # Train the model on the combined data\n",
        "    model.fit(X_combined_transformed)\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    joblib.dump((model, title_tfidf, category_tfidf), '/content/gdrive/My Drive/Recommendation/Datasets/k_means_model.joblib')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    X_val_transformed = hstack([title_tfidf.transform(val_data['title']), category_tfidf.transform(val_data['categoryName'])])\n",
        "    predictions = model.predict(X_val_transformed)\n",
        "\n",
        "    # Calculate the Silhouette Score which measures how well-defined the clusters are in the data.\n",
        "    # It ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "    silhouette_avg = silhouette_score(X_combined_transformed, model.named_steps['kmeans'].labels_)\n",
        "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "# Recommend products based on the input\n",
        "def recommend_product(title, category, model_path, train_data, X_combined_transformed, num_recommendations=1):\n",
        "    # Load the pre-trained model\n",
        "    model, title_tfidf, category_tfidf = joblib.load(model_path)\n",
        "\n",
        "    # Transform input data using the TF-IDF transformers\n",
        "    input_features = transform_input(title, category, title_tfidf, category_tfidf)\n",
        "\n",
        "    # Predict the cluster for the input data\n",
        "    cluster = model.named_steps['kmeans'].predict(input_features)\n",
        "\n",
        "    # Find products in the same cluster as the input data\n",
        "    labels = model.named_steps['kmeans'].labels_\n",
        "    cluster_indices = np.where(labels == cluster[0])[0]\n",
        "\n",
        "    # Calculate the cosine similarity between the input product and each product in the cluster and sort them based on these similarity scores.\n",
        "    # The resulting recommended_titles list will contain the best options within the same cluster.\n",
        "    similarity_scores = []\n",
        "    for index in cluster_indices:\n",
        "        similarity_score = np.dot(input_features.toarray(), X_combined_transformed[index].toarray().T)[0, 0]\n",
        "        similarity_scores.append((index, similarity_score))\n",
        "\n",
        "    # Sort products based on similarity scores in descending order\n",
        "    sorted_products = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract recommended titles\n",
        "    recommended_titles = [train_data.iloc[index]['title'] for index, _ in sorted_products]\n",
        "\n",
        "    # Return only the top N recommendations\n",
        "    return recommended_titles[:num_recommendations]\n",
        "\n",
        "# Make recommendations updating the CSV file\n",
        "def get_recommendations(model_path, train_data, recommendation_column):\n",
        "    file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/recommendations.csv\"\n",
        "\n",
        "    # Create a temporary file to write the updated data\n",
        "    temp_file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/temp_recommendations.csv\"\n",
        "\n",
        "    with open(file_path, 'r') as infile, open(temp_file_path, 'w', newline='') as outfile:\n",
        "        csv_reader = csv.DictReader(infile)\n",
        "        fieldnames = csv_reader.fieldnames  # Retrieve the existing field names\n",
        "\n",
        "        # Add the recommendation_column to the list of fieldnames if not already present\n",
        "        if recommendation_column not in fieldnames:\n",
        "            fieldnames.append(recommendation_column)\n",
        "\n",
        "        csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        for row in csv_reader:\n",
        "            title = row['title']\n",
        "            categoryName = row['categoryName']\n",
        "            recommended_titles = recommend_product(title, categoryName, model_path, train_data, X_combined_transformed, 1)\n",
        "            prediction_value = recommended_titles[0]\n",
        "\n",
        "            # Update the existing 'kMeansRecommendation' column in each row\n",
        "            row[recommendation_column] = prediction_value\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    # Replace the original file with the temporary file\n",
        "    os.replace(temp_file_path, file_path)\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/train_set.csv')\n",
        "validation_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/validation_set.csv')\n",
        "\n",
        "# Train the model with both training and validation datasets\n",
        "train_model(train_data, validation_data)\n",
        "\n",
        "# train_data includes both training and validation data\n",
        "train_data_combined = pd.concat([train_data, validation_data], ignore_index=True)\n",
        "X_combined_transformed = hstack([\n",
        "    title_tfidf.transform(train_data_combined['title']),\n",
        "    category_tfidf.transform(train_data_combined['categoryName'])\n",
        "])\n",
        "\n",
        "# Call the function with the appropriate arguments\n",
        "get_recommendations('/content/gdrive/My Drive/Recommendation/Datasets/k_means_model.joblib', train_data_combined, 'kMeansRecommendation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epIEXa6pHNwQ",
        "outputId": "76d783c2-a579-4ba0-805a-8ef3586fe041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.12953924791119417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content-Based Filtering (CBF)\n",
        "\n",
        "Content-Based Filtering doesn't involve a separate training phase like collaborative filtering or some machine learning models. Instead, it relies on creating representations of items (in this case, product titles and categories) and making recommendations based on the similarity of these representations.\n",
        "\n",
        "In the provided code, the training process involves creating TF-IDF representations for product titles and categories using TfidfVectorizer. The vectorizers (title_tfidf and category_tfidf) are then used to transform the training data during recommendation.\n",
        "\n",
        "In these functions, the TF-IDF vectorizers (title_tfidf and category_tfidf) are used to transform input titles, categories, and titles/categories in the training data into numerical feature vectors. The recommend_product_cbf function then calculates the similarity scores using these feature vectors for recommendation."
      ],
      "metadata": {
        "id": "47wYx7UfHJWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZustp8lEX_N",
        "outputId": "e86ded24-f5af-46d8-a003-a955d5ee8a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive to access and save files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Transform input data using pre-trained TF-IDF transformers\n",
        "# transform_input: This function takes a product title, category, and the trained TF-IDF vectorizers (title_tfidf and category_tfidf) and transforms the input title and category into a feature vector that can be used for recommendation.\n",
        "def transform_input(title, category, title_tfidf, category_tfidf):\n",
        "    title_transformed = title_tfidf.transform([title])\n",
        "    category_transformed = category_tfidf.transform([category])\n",
        "    return np.hstack([title_transformed.toarray(), category_transformed.toarray()])\n",
        "\n",
        "# Train the TF-IDF transformers\n",
        "def train_tfidf_transformers(train_data):\n",
        "    title_tfidf = TfidfVectorizer().fit(train_data['title'])\n",
        "    category_tfidf = TfidfVectorizer().fit(train_data['categoryName'])\n",
        "    return title_tfidf, category_tfidf\n",
        "\n",
        "# Recommend products based on the input using Content-Based Filtering\n",
        "# recommend_product_cbf: This function takes an input title, category, and the TF-IDF vectorizers along with the training data. It calculates the cosine similarity between the input product and each product in the training data based on their TF-IDF representations. The products are then sorted by similarity scores, and a list of recommended titles is returned.\n",
        "def recommend_product_cbf(title, category, title_tfidf, category_tfidf, train_data, num_recommendations=1):\n",
        "    # Transform input data using the TF-IDF transformers\n",
        "    input_features = transform_input(title, category, title_tfidf, category_tfidf)\n",
        "\n",
        "    # Transform titles and categories in the training data\n",
        "    train_data_transformed = np.hstack([\n",
        "        title_tfidf.transform(train_data['title']).toarray(),\n",
        "        category_tfidf.transform(train_data['categoryName']).toarray()\n",
        "    ])\n",
        "\n",
        "    # Calculate cosine similarity between the input product and each product in the training data\n",
        "    similarity_scores = linear_kernel(input_features.reshape(1, -1), train_data_transformed).flatten()\n",
        "\n",
        "    # Sort products based on similarity scores in descending order\n",
        "    sorted_products = np.argsort(similarity_scores)[::-1]\n",
        "\n",
        "    # Extract recommended titles\n",
        "    recommended_titles = [train_data.iloc[index]['title'] for index in sorted_products]\n",
        "\n",
        "    # Return only the top N recommendations\n",
        "    return recommended_titles[:num_recommendations]\n",
        "\n",
        "# Make recommendations updating the CSV file using Content-Based Filtering\n",
        "# get_cbf_recommendations: This function updates the CSV file with recommendations based on Content-Based Filtering. It uses the recommend_product_cbf function to get recommendations for each product in the file.\n",
        "def get_cbf_recommendations(train_data, recommendation_column):\n",
        "    file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/recommendations.csv\"\n",
        "\n",
        "    # Create a temporary file to write the updated data\n",
        "    temp_file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/temp_recommendations.csv\"\n",
        "\n",
        "    # Train TF-IDF transformers\n",
        "    title_tfidf, category_tfidf = train_tfidf_transformers(train_data)\n",
        "\n",
        "    with open(file_path, 'r') as infile, open(temp_file_path, 'w', newline='') as outfile:\n",
        "        csv_reader = csv.DictReader(infile)\n",
        "        fieldnames = csv_reader.fieldnames  # Retrieve the existing field names\n",
        "\n",
        "        # Add the recommendation_column to the list of fieldnames if not already present\n",
        "        if recommendation_column not in fieldnames:\n",
        "            fieldnames.append(recommendation_column)\n",
        "\n",
        "        csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        for row in csv_reader:\n",
        "            title = row['title']\n",
        "            category = row['categoryName']\n",
        "            recommended_titles = recommend_product_cbf(title, category, title_tfidf, category_tfidf, train_data, 1)\n",
        "            prediction_value = recommended_titles[0]\n",
        "\n",
        "            # Update the existing 'cbfRecommendation' column in each row\n",
        "            row[recommendation_column] = prediction_value\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    # Replace the original file with the temporary file\n",
        "    os.replace(temp_file_path, file_path)\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/train_set.csv')\n",
        "validation_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/validation_set.csv')\n",
        "\n",
        "# Call the function with the appropriate arguments\n",
        "get_cbf_recommendations(train_data, 'cbfRecommendation')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering"
      ],
      "metadata": {
        "id": "n1_HnWFzIL5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive to access and save files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define global variables for TF-IDF transformers\n",
        "title_tfidf = None\n",
        "category_tfidf = None\n",
        "\n",
        "# Transform input data using pre-trained TF-IDF transformers\n",
        "def transform_input(title, category, title_tfidf, category_tfidf):\n",
        "    title_transformed = title_tfidf.transform([title])\n",
        "    category_transformed = category_tfidf.transform([category])\n",
        "    return hstack([title_transformed, category_transformed])\n",
        "\n",
        "# Train the Hierarchical Clustering model\n",
        "def train_model(train_data, val_data):\n",
        "    global title_tfidf, category_tfidf  # Declare as global variables\n",
        "\n",
        "    # Combine training and validation datasets for training\n",
        "    combined_data = pd.concat([train_data, val_data], axis=0, ignore_index=True)\n",
        "\n",
        "    # Split your combined data into features (X) and labels (y)\n",
        "    X_combined = combined_data[['title', 'categoryName']]\n",
        "\n",
        "    # Feature extraction using TF-IDF for title and categoryName\n",
        "    title_tfidf = TfidfVectorizer().fit(X_combined['title'])\n",
        "    category_tfidf = TfidfVectorizer().fit(X_combined['categoryName'])\n",
        "\n",
        "    # Combine features into a single matrix for training\n",
        "    title_transformed = title_tfidf.transform(X_combined['title'])\n",
        "    category_transformed = category_tfidf.transform(X_combined['categoryName'])\n",
        "    X_combined_transformed = hstack([title_transformed, category_transformed])\n",
        "\n",
        "    # Choose the number of clusters (make sure it's at least 2)\n",
        "    num_clusters = 2\n",
        "    model = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "\n",
        "    # Train the model on the combined data\n",
        "    model.fit(X_combined_transformed.toarray())\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    joblib.dump((model, title_tfidf, category_tfidf), '/content/gdrive/My Drive/Recommendation/Datasets/hierarchical_model.joblib')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    predictions = model.labels_\n",
        "\n",
        "    # Calculate the Silhouette Score\n",
        "    silhouette_avg = silhouette_score(X_combined_transformed.toarray(), predictions)\n",
        "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Recommend products based on the input\n",
        "def recommend_product(title, category, model, train_data_combined, X_combined_transformed, num_recommendations=1):\n",
        "    # Transform input data using the TF-IDF transformers\n",
        "    input_features = transform_input(title, category, title_tfidf, category_tfidf)\n",
        "\n",
        "    # Predict the cluster for the input data\n",
        "    cluster = model.labels_\n",
        "\n",
        "    # Find products in the same cluster as the input data\n",
        "    cluster_indices = np.where(cluster == cluster[0])[0]\n",
        "\n",
        "    # Calculate the cosine similarity between the input product and each product in the cluster and sort them based on these similarity scores.\n",
        "    # The resulting recommended_titles list will contain the best options within the same cluster.\n",
        "    similarity_scores = []\n",
        "    for index in cluster_indices:\n",
        "        similarity_score = np.dot(input_features.toarray(), X_combined_transformed[index].toarray().T)[0, 0]\n",
        "        similarity_scores.append((index, similarity_score))\n",
        "\n",
        "    # Sort products based on similarity scores in descending order\n",
        "    sorted_products = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract recommended titles\n",
        "    recommended_titles = [train_data_combined.iloc[index]['title'] for index, _ in sorted_products]\n",
        "\n",
        "    # Return only the top N recommendations\n",
        "    return recommended_titles[:num_recommendations]\n",
        "\n",
        "# Make recommendations updating the CSV file\n",
        "def get_recommendations(model_path, train_data_combined, recommendation_column, X_combined_transformed):\n",
        "    # Load the pre-trained model\n",
        "    model, title_tfidf, category_tfidf = joblib.load(model_path)\n",
        "\n",
        "    # Call the function with the appropriate arguments\n",
        "    recommended_titles = recommend_product(\n",
        "        train_data_combined['title'][0],  # Replace 0 with the index of the specific row you want to recommend for\n",
        "        train_data_combined['categoryName'][0],  # Replace 0 with the index of the specific row you want to recommend for\n",
        "        model,\n",
        "        train_data_combined,\n",
        "        X_combined_transformed,\n",
        "        1\n",
        "    )\n",
        "\n",
        "    print(f'Recommended titles: {recommended_titles}')\n",
        "\n",
        "    file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/recommendations.csv\"\n",
        "\n",
        "    # Create a temporary file to write the updated data\n",
        "    temp_file_path = \"/content/gdrive/My Drive/Recommendation/Datasets/temp_recommendations.csv\"\n",
        "\n",
        "    with open(file_path, 'r') as infile, open(temp_file_path, 'w', newline='') as outfile:\n",
        "        csv_reader = csv.DictReader(infile)\n",
        "        fieldnames = csv_reader.fieldnames  # Retrieve the existing field names\n",
        "\n",
        "        # Add the recommendation_column to the list of fieldnames if not already present\n",
        "        if recommendation_column not in fieldnames:\n",
        "            fieldnames.append(recommendation_column)\n",
        "\n",
        "        csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        csv_writer.writeheader()\n",
        "\n",
        "        for row in csv_reader:\n",
        "            title = row['title']\n",
        "            categoryName = row['categoryName']\n",
        "            recommended_titles = recommend_product(title, categoryName, model, train_data_combined, X_combined_transformed, 1)\n",
        "            prediction_value = recommended_titles[0]\n",
        "\n",
        "            # Update the existing 'hierarchicalRecommendation' column in each row\n",
        "            row[recommendation_column] = prediction_value\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "    # Replace the original file with the temporary file\n",
        "    os.replace(temp_file_path, file_path)\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/train_set.csv')\n",
        "validation_data = pd.read_csv('/content/gdrive/My Drive/Recommendation/Datasets/validation_set.csv')\n",
        "\n",
        "# Train the model with both training and validation datasets\n",
        "train_model(train_data, validation_data)\n",
        "\n",
        "# train_data includes both training and validation data\n",
        "train_data_combined = pd.concat([train_data, validation_data], ignore_index=True)\n",
        "X_combined_transformed = hstack([\n",
        "    title_tfidf.transform(train_data_combined['title']),\n",
        "    category_tfidf.transform(train_data_combined['categoryName'])\n",
        "])\n",
        "\n",
        "# Call the function with the appropriate arguments\n",
        "get_recommendations('/content/gdrive/My Drive/Recommendation/Datasets/hierarchical_model.joblib', train_data_combined, 'hierarchicalRecommendation', X_combined_transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nbD4DzJIMpA",
        "outputId": "f914d915-fe51-4174-8c37-70ce7fcb3339"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Silhouette Score: 0.11334528177046156\n",
            "Recommended titles: ['HYZUO Laptop Backpack with USB Charging Port Anti-Theft Water Resistant Slim Stylish College School Backpack Business Travel Bag Fits Up to 15.6 Inch Laptop for Men and Women, Light Grey']\n"
          ]
        }
      ]
    }
  ]
}